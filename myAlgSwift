//: Playground - noun: a place where people can play

import UIKit

// CREDIT : https://github.com/raywenderlich/swift-algorithm-club

// Big-O Notation

//Big-O             Name                Description

//O(1)              constant            This is the best. The algorithm always takes the same amount of time,regardless of how much data there is. Example: looking up an element of an array by its index.

//O(log n)          logarithmic         Pretty great. These kinds of algorithms halve the amount of data with each iteration. If you have 100 items, it takes about 7 steps to find the answer. With 1,000 items, it takes 10 steps. And 1,000,000 items only take 20 steps. This is super fast even for large amounts of data. Example: binary search.

//O(n)              linear              Good performance. If you have 100 items, this does 100 units of work. Doubling the number of items makes the algorithm take exactly twice as long (200 units of work). Example: sequential search.

//O(n log n)        "linearithmic"      Decent performance. This is slightly worse than linear but not too bad. Example: the fastest general-purpose sorting algorithms.

//O(n^2)            quadratic           Kinda slow. If you have 100 items, this does 100^2 = 10,000 units of work. Doubling the number of items makes it four times slower (because 2 squared equals 4). Example: algorithms using nested loops, such as insertion sort.

//O(n^3)            cubic               Poor performance. If you have 100 items, this does 100^3 = 1,000,000 units of work. Doubling the input size makes it eight times slower. Example: matrix multiplication.

//O(2^n)            exponential         Very poor performance. You want to avoid these kinds of algorithms, but sometimes you have no choice. Adding just one bit to the input doubles the running time. Example: traveling salesperson problem.

//O(n!)             factorial           Intolerably slow. It literally takes a million years to do anything.


//Stack is LIFO - 
//  Array is expensive, an O(n) operation, because it requires all existing array elements to be shifted in memory. Adding at the end is O(1); it always takes the same amount of time, regardless of the size of the array.
//
//Fun fact about stacks: Each time you call a function or a method, the CPU places the return address on a stack. When the function ends, the CPU uses that return address to jump back to the caller. That's why if you call too many functions -- for example in a recursive function that never ends -- you get a so-called "stack overflow" as the CPU stack has run out of space.

public struct TestStack<T> {
    fileprivate var array = [T]()
    
    public var isEmpty: Bool {
        return array.isEmpty
    }
    
    public var count: Int {
        return array.count
    }
    
    public mutating func push(_ element: T) {
        array.append(element)
    }
    
    public mutating func pop() -> T? {
        
        if isEmpty {
            
            return nil
        }
        return array.popLast()
    }
    
    public var top: T? {
        return array.last
    }
}

var addStack = TestStack.init(array: [])
print(addStack)
print(addStack.count)
print(addStack.push(1))
print(addStack.pop()!)
print(addStack.push(2))
print(addStack.push(3))
print(addStack.top!)
